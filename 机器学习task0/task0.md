# Task 0: 机器学习入门

> 由于我之前学过一些深度学习的内容，故此处不再展示之前的笔记，请见：[Althra/learn-deeplearning](https://github.com/Althra/learn-deeplearning)

以下内容为题中问题的回答：

### 1.监督学习与无监督学习的区别

- 监督学习：需要有标签的数据集作为输入，也就是说，每一个输入样本都对应一个已知的输出。目标是通过这些已知的输入-输出对，训练一个模型，使其能够对新输入进行预测。常见的监督学习算法有线性回归、逻辑回归和神经网络等。
  
- 无监督学习：不需要标签的数据集，模型通过对数据的结构和规律进行学习，找到隐藏的模式或数据结构。常见的无监督学习算法包括聚类、降维和关联规则学习等。

### 2. 机器学习和深度学习的区别
- 机器学习：是一种使用统计学原理从数据中学习的技术，主要包括监督学习、无监督学习和强化学习。常用算法有回归分析、决策树、随机森林、支持向量机等。机器学习模型可以是较简单的算法，也可以是复杂的模型，如神经网络。

- 深度学习：是机器学习的一个子领域，主要关注于使用深层神经网络（DNN）来模拟人脑的工作方式。深度学习在处理图像、语音、文本等非结构化数据时表现特别出色，依赖于大量的数据和计算资源。它的核心是通过多层神经元层级，进行特征提取和表示。

### 3. 偏导数、链式法则、梯度、矩阵等数学概念在机器学习中的作用
- 偏导数：在机器学习中，用于计算损失函数对各个参数的变化率，以便在优化过程中调整这些参数。
- 链式法则：在反向传播中，用于计算损失函数对模型参数的梯度。链式法则可以处理由多层神经元构成的复合函数的导数计算。
- 梯度：表示函数在某一点的最陡上升方向。梯度下降法是优化算法，通过反向传播计算梯度，然后逐步调整模型参数，以使损失函数达到最小值。
- 矩阵：在机器学习中，用于表示数据集和权重等，神经网络的运算中大量使用矩阵。矩阵运算可以加速批量数据的处理，并利用硬件（如GPU）进行并行计算。

### 4. 常见的激活函数
- Sigmoid: 输出值在(0, 1)之间，是一个平滑的、可微的阈值单元近似。
- Tanh: 输出值在(-1, 1)之间，相较于Sigmoid有更强的梯度。
- ReLU (Rectified Linear Unit): 输出值为正则输出本身，负则输出0。计算简单，解决了一部分梯度消失问题，但有“死亡ReLU”的问题（即某些神经元一直输出0）。
- Leaky ReLU: 对ReLU的改进，允许负数部分有一个很小的斜率，减轻了“死亡ReLU”的问题。
- Softmax: 通常用于多分类问题的最后一层，将输出转换为概率分布。

### 5. 神经网络的基本结构
- 输入层：接收输入数据，每个节点代表输入特征。
- 隐藏层：在输入层和输出层之间的层，负责特征提取与组合。隐藏层越多越复杂，可以表示的函数越复杂。
- 输出层：输出模型的预测值，节点数取决于预测任务（如分类任务的类别数）。
- 权重和偏置：连接层之间的参数，决定了输入信号如何被转换和传递。
- 激活函数：引入非线性，使神经网络能够学习复杂的模式和特征。

### 6. 机器学习中的数据处理
- 数据预处理：
  - 标准化：将数据转换为均值为0，方差为1的分布，有助于梯度下降更快收敛。
  - 归一化：将数据缩放到一定范围（如0到1），适用于不同量纲的数据。
- 数据清洗：包括去除空值、异常值、处理重复数据等，确保数据的质量。
- 特征选择：选择最有用的特征以减少噪声，提高模型的性能。
- 数据增强：特别是在CV等领域，通过生成新的训练样本来提升模型的泛化能力。
- 数据划分：将数据集划分为训练集、验证集和测试集，分别用于模型训练、调优和性能评估。
