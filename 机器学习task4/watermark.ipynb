{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabef06b-937b-4dc6-91b5-b58e8e45dffb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-23T15:06:33.215027Z",
     "iopub.status.busy": "2024-10-23T15:06:33.214832Z",
     "iopub.status.idle": "2024-10-23T15:06:34.851637Z",
     "shell.execute_reply": "2024-10-23T15:06:34.850841Z",
     "shell.execute_reply.started": "2024-10-23T15:06:33.215002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, random\n",
    "import numpy as np\n",
    "from markllm.watermark.auto_watermark import AutoWatermark\n",
    "from markllm.utils.transformers_config import TransformersConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f849f83-21a4-4b76-b0c2-8bf41d01bf69",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-23T15:06:36.320020Z",
     "iopub.status.busy": "2024-10-23T15:06:36.319663Z",
     "iopub.status.idle": "2024-10-23T15:06:39.618083Z",
     "shell.execute_reply": "2024-10-23T15:06:39.617441Z",
     "shell.execute_reply.started": "2024-10-23T15:06:36.319994Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed for reproducibility\n",
    "seed = 30\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Transformers config\n",
    "model_name = './gpt2'\n",
    "transformers_config = TransformersConfig(\n",
    "    model=AutoModelForCausalLM.from_pretrained(model_name).to(device),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "    device=device,\n",
    "    max_new_tokens=200,\n",
    "    min_length=230\n",
    ")\n",
    "\n",
    "# Load watermark algorithm\n",
    "myWatermark = AutoWatermark.load('KGW', transformers_config=transformers_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acaedd58-46c9-4ffd-acdc-cd30968f0484",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-23T15:06:43.898037Z",
     "iopub.status.busy": "2024-10-23T15:06:43.897567Z",
     "iopub.status.idle": "2024-10-23T15:06:47.621111Z",
     "shell.execute_reply": "2024-10-23T15:06:47.620472Z",
     "shell.execute_reply.started": "2024-10-23T15:06:43.898010Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1257: UserWarning: Unfeasible length constraints: `min_length` (230) is larger than the maximum possible length (204). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 204, its default value.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watermarked_text: Who are you? Are you going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're going to tell me you're\n",
      "unwatermarked_text: Who are you?\n",
      "\n",
      "I'm a guy who's been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for a long time. I've been in the business for\n"
     ]
    }
   ],
   "source": [
    "# Prompt and generation\n",
    "prompt = 'Who are you?'\n",
    "watermarked_text = myWatermark.generate_watermarked_text(prompt)\n",
    "unwatermarked_text = myWatermark.generate_unwatermarked_text(prompt)\n",
    "\n",
    "print(f\"watermarked_text: {watermarked_text}\")\n",
    "print(f\"unwatermarked_text: {unwatermarked_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9259c7b7-1f50-4a56-be50-6304862a7450",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-23T15:06:53.354287Z",
     "iopub.status.busy": "2024-10-23T15:06:53.353876Z",
     "iopub.status.idle": "2024-10-23T15:06:53.395983Z",
     "shell.execute_reply": "2024-10-23T15:06:53.395170Z",
     "shell.execute_reply.started": "2024-10-23T15:06:53.354259Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_watermarked': True, 'score': 14.107434367506288}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detection\n",
    "myWatermark.detect_watermark(watermarked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083eccac-7ab0-4987-9386-1c6a068c85d5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-23T15:06:56.832553Z",
     "iopub.status.busy": "2024-10-23T15:06:56.832164Z",
     "iopub.status.idle": "2024-10-23T15:06:56.872902Z",
     "shell.execute_reply": "2024-10-23T15:06:56.872135Z",
     "shell.execute_reply.started": "2024-10-23T15:06:56.832523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_watermarked': False, 'score': -3.4391257910836224}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myWatermark.detect_watermark(unwatermarked_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
